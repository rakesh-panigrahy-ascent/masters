{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Seq2Seq LSTM Model - Translation_with_prediction_Attention.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JoJPolOgWGG4","colab_type":"text"},"source":["### Load tensorflow"]},{"cell_type":"code","metadata":{"id":"zlt_1eHPWGG6","colab_type":"code","outputId":"c7f7274e-0d9c-465b-af63-7aa1918a1c14","executionInfo":{"status":"ok","timestamp":1578827658934,"user_tz":-330,"elapsed":3158,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":81}},"source":["import tensorflow as tf\n","tf.reset_default_graph()\n","tf.set_random_seed(42)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"VzxABbMJWGG-","colab_type":"text"},"source":["### Read the data\n","<font size=\"2\">Data for this exercise can be downloaded from http://www.manythings.org/anki/</font>"]},{"cell_type":"code","metadata":{"id":"ommjMLuF75Dk","colab_type":"code","outputId":"ebf03a27-f816-4ba5-e10c-66838546e3b8","executionInfo":{"status":"ok","timestamp":1578827661268,"user_tz":-330,"elapsed":5469,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1vDoJM0zWGG_","colab_type":"code","outputId":"1c797d05-8816-470c-ca02-9793848b0f14","executionInfo":{"status":"ok","timestamp":1578827663386,"user_tz":-330,"elapsed":7558,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["#You can use wget to download the file directly\n","!wget http://www.manythings.org/anki/hin-eng.zip"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2020-01-12 11:14:19--  http://www.manythings.org/anki/hin-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6dc4, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 126500 (124K) [application/zip]\n","Saving to: ‘hin-eng.zip’\n","\n","\rhin-eng.zip           0%[                    ]       0  --.-KB/s               \rhin-eng.zip          86%[================>   ] 106.57K   439KB/s               \rhin-eng.zip         100%[===================>] 123.54K   508KB/s    in 0.2s    \n","\n","2020-01-12 11:14:20 (508 KB/s) - ‘hin-eng.zip’ saved [126500/126500]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A8soXgFrWGHB","colab_type":"code","colab":{}},"source":["import zipfile\n","import io\n","\n","#Read the zip file\n","zf = zipfile.ZipFile('hin-eng.zip', 'r')\n","\n","#Extract data from zip file\n","data = ''\n","with zf.open('hin.txt') as readfile:\n","  for line in io.TextIOWrapper(readfile, 'utf-8'):\n","    data += line"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQFmYMvhWGHE","colab_type":"code","outputId":"475e0a26-7681-4933-8cc4-67e85e3be33c","executionInfo":{"status":"ok","timestamp":1578827663388,"user_tz":-330,"elapsed":7532,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["data[400:500]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #6179123 (fastrizwaan)\\nHello!\\tनमस्'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"lx5AWe-0WGHT","colab_type":"text"},"source":["\n","### Extract Source and Target Language pairs"]},{"cell_type":"code","metadata":{"id":"mjma2IuuWGHU","colab_type":"code","outputId":"6d1c7752-3c1b-41e0-baaa-48b21a7b71b3","executionInfo":{"status":"ok","timestamp":1578827663389,"user_tz":-330,"elapsed":7521,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["#Split by newline character\n","data =  data.split('\\n')\n","\n","#Show some Data\n","data[100:105]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I have a car.\\tमेरे पास एक गाड़ी है।\\tCC-BY 2.0 (France) Attribution: tatoeba.org #252272 (CK) & #477720 (minshirui)',\n"," 'I have a dog.\\tमेरे पास एक कुत्ता है।\\tCC-BY 2.0 (France) Attribution: tatoeba.org #378502 (CK) & #443037 (minshirui)',\n"," 'I understand.\\tमैं समझता हूँ।\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433468 (CK) & #588495 (minshirui)',\n"," \"I'm a doctor.\\tमैं डॉक्टर हूँ।\\tCC-BY 2.0 (France) Attribution: tatoeba.org #256018 (CK) & #449296 (minshirui)\",\n"," 'It is a book.\\tयह किताब है।\\tCC-BY 2.0 (France) Attribution: tatoeba.org #42097 (CK) & #443050 (minshirui)']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Af9jau2XWGHX","colab_type":"code","outputId":"7a319537-1477-49c8-f2f3-2f27f1893e02","executionInfo":{"status":"ok","timestamp":1578827663391,"user_tz":-330,"elapsed":7513,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(data)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2780"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"A4ow84XDWGHb","colab_type":"text"},"source":["### Separate Source and Target pairs"]},{"cell_type":"code","metadata":{"id":"GTzjK4G6WGHb","colab_type":"code","colab":{}},"source":["encoder_text = [] #Initialize Source language list\n","decoder_text = [] #Initialize Target language list\n","\n","#Iterate over data\n","for line in data:\n","    try:\n","        in_txt, out_txt,_ = line.split('\\t')\n","        encoder_text.append(in_txt)\n","        \n","        # Add tab '<start>' as 'start sequence in target\n","        # And '<end>' as End\n","        decoder_text.append('<start> ' + out_txt + ' <end>')\n","    except:\n","        pass #ignore data which goes into error        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4T4HJ3A9WGHd","colab_type":"text"},"source":["### Separate Source and Target pairs.."]},{"cell_type":"code","metadata":{"id":"xyZqwGuvWGHe","colab_type":"code","outputId":"a007b496-cad1-40c9-96b3-4156f6cd17a6","executionInfo":{"status":"ok","timestamp":1578827663392,"user_tz":-330,"elapsed":7497,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":103}},"source":["encoder_text[100:105]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I have a car.',\n"," 'I have a dog.',\n"," 'I understand.',\n"," \"I'm a doctor.\",\n"," 'It is a book.']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"v2CILOS3WGHg","colab_type":"code","outputId":"27963750-f317-4900-d654-914ec30186a6","executionInfo":{"status":"ok","timestamp":1578827663393,"user_tz":-330,"elapsed":7483,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":103}},"source":["decoder_text[100:105]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> मेरे पास एक गाड़ी है। <end>',\n"," '<start> मेरे पास एक कुत्ता है। <end>',\n"," '<start> मैं समझता हूँ। <end>',\n"," '<start> मैं डॉक्टर हूँ। <end>',\n"," '<start> यह किताब है। <end>']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"CnRP431qWGHj","colab_type":"text"},"source":["### Tokenize Source language sentences"]},{"cell_type":"code","metadata":{"id":"9ucqBYr4WGHk","colab_type":"code","outputId":"9ccf1e8e-81f8-4068-d11a-0bb9fb5a251a","executionInfo":{"status":"ok","timestamp":1578827663720,"user_tz":-330,"elapsed":7801,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Tokenizer for source language\n","encoder_t = tf.keras.preprocessing.text.Tokenizer()\n","encoder_t.fit_on_texts(encoder_text) #Fit it on Source sentences\n","encoder_seq = encoder_t.texts_to_sequences(encoder_text) #Convert sentences to numbers \n","encoder_seq[100:105] #Display some converted sentences"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[2, 14, 6, 96], [2, 14, 6, 124], [2, 208], [39, 6, 150], [10, 5, 6, 69]]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"zZv-TPVOWGHn","colab_type":"code","outputId":"856200b6-5a9d-492f-b491-5de601e69352","executionInfo":{"status":"ok","timestamp":1578827663721,"user_tz":-330,"elapsed":7792,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Maximum length of sentence\n","max_encoder_seq_length = max([len(txt) for txt in encoder_seq])\n","print('Maximum sentence length for Source language: ', max_encoder_seq_length)\n","\n","#Source language Vocablury\n","encoder_vocab_size = len(encoder_t.word_index)\n","print('Source language vocablury size: ', encoder_vocab_size)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Maximum sentence length for Source language:  22\n","Source language vocablury size:  2375\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BeASC50tWGHr","colab_type":"text"},"source":["### Tokenize Target language sentences"]},{"cell_type":"code","metadata":{"id":"Zcyow_keWGHr","colab_type":"code","colab":{}},"source":["#Tokenizer for target language, filters should not <start> and <end>\n","#remove < and > used in Target language sequences\n","decoder_t = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","decoder_t.fit_on_texts(decoder_text) #Fit it on target sentences\n","decoder_seq = decoder_t.texts_to_sequences(decoder_text) #Convert sentences to numbers "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"phLrIK6WWGHt","colab_type":"code","outputId":"9771b40e-3372-404c-b1ca-395fe9fcdc56","executionInfo":{"status":"ok","timestamp":1578827663722,"user_tz":-330,"elapsed":7781,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Maximum length of sentence\n","max_decoder_seq_length = max([len(txt) for txt in decoder_seq])\n","print('Maximum sentence length for Target language: ', max_decoder_seq_length)\n","\n","#Target language Vocablury\n","decoder_vocab_size = len(decoder_t.word_index)\n","print('Target language vocablury size: ', decoder_vocab_size)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Maximum sentence length for Target language:  27\n","Target language vocablury size:  2973\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6fXf48uMWGH6","colab_type":"text"},"source":["### Compare different sentences length"]},{"cell_type":"code","metadata":{"id":"w0aRkCfiWGH7","colab_type":"code","outputId":"83ca87fb-9e74-485b-f241-e428704ff13a","executionInfo":{"status":"ok","timestamp":1578827663724,"user_tz":-330,"elapsed":7595,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Source Language sentences\n","print('Length for sentence number 100: ', len(encoder_seq[100]))\n","print('Length for sentence number 2000: ', len(encoder_seq[2000]))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Length for sentence number 100:  4\n","Length for sentence number 2000:  8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W2qZN4BvWGH_","colab_type":"code","outputId":"49664a19-20db-401d-d789-ae9c24b942aa","executionInfo":{"status":"ok","timestamp":1578827663724,"user_tz":-330,"elapsed":7079,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Target Language sentences\n","print('Length for sentence number 100: ', len(decoder_seq[100]))\n","print('Length for sentence number 2000: ', len(decoder_seq[2000]))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Length for sentence number 100:  7\n","Length for sentence number 2000:  8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wmeSCSs6WGIC","colab_type":"text"},"source":["### How do we make it same?"]},{"cell_type":"markdown","metadata":{"id":"mqps8juMWGIE","colab_type":"text"},"source":["### Padding the sentences"]},{"cell_type":"code","metadata":{"id":"8rnbyRs9WGIF","colab_type":"code","colab":{}},"source":["#Source sentences\n","encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_seq, \n","                                                                   maxlen=max_encoder_seq_length, #22\n","                                                                   padding='pre')\n","\n","#Target Sentences\n","decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_seq, \n","                                                                   maxlen=max_decoder_seq_length, #27\n","                                                                   padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a61g_ADpWGIH","colab_type":"code","outputId":"47f6b6d2-4446-480c-e1dd-50580740fe11","executionInfo":{"status":"ok","timestamp":1578827671958,"user_tz":-330,"elapsed":836,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print('Source data shape: ', encoder_input_data.shape)\n","print('Target data shape: ', decoder_input_data.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Source data shape:  (2779, 22)\n","Target data shape:  (2779, 27)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nl4oxg8cWGIJ","colab_type":"text"},"source":["#### Integer to Word converter for Decoder data"]},{"cell_type":"code","metadata":{"id":"jhwnBD-0WGIK","colab_type":"code","colab":{}},"source":["int_to_word_decoder = dict((i,c) for c, i in decoder_t.word_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8TP07uxWGIP","colab_type":"code","outputId":"7638eb0b-8240-4656-e484-499da8592fb5","executionInfo":{"status":"ok","timestamp":1578827674294,"user_tz":-330,"elapsed":729,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["int_to_word_decoder[15]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'की'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"5g4Y4oRvWGIV","colab_type":"text"},"source":["### Building Decoder Output"]},{"cell_type":"code","metadata":{"id":"YuoPA9aWWGIV","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","#Initialize array\n","decoder_target_data = np.zeros((decoder_input_data.shape[0], decoder_input_data.shape[1]))\n","\n","#Shift Target output by one word\n","for i in range(decoder_input_data.shape[0]):\n","    for j in range(1,decoder_input_data.shape[1]):\n","        decoder_target_data[i][j-1] = decoder_input_data[i][j]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"haordQCuWGIX","colab_type":"text"},"source":["#### Convert target data in one hot vector"]},{"cell_type":"code","metadata":{"id":"Vs2SKKI5WGIY","colab_type":"code","colab":{}},"source":["#Initialize one hot encoding array\n","decoder_target_one_hot = np.zeros((decoder_input_data.shape[0], #number of sentences\n","                                   decoder_input_data.shape[1], #Number of words in each sentence\n","                                   len(decoder_t.word_index)+1)) #Vocab size + 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoX_6OP4WGIm","colab_type":"code","colab":{}},"source":["#Build one hot encoded array\n","for i in range(decoder_target_data.shape[0]):\n","    for j in range(decoder_target_data.shape[1]):\n","        decoder_target_one_hot[i][j] = tf.keras.utils.to_categorical(decoder_target_data[i][j],\n","                                                                     num_classes=len(decoder_t.word_index)+1)    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5fO1TTWWGIo","colab_type":"code","outputId":"fc91d34c-993c-4379-830f-6fa393064a44","executionInfo":{"status":"ok","timestamp":1578827678984,"user_tz":-330,"elapsed":1823,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["decoder_target_one_hot.shape"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2779, 27, 2974)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"fQYFym0AWGIq","colab_type":"text"},"source":["### Building the Training Model"]},{"cell_type":"code","metadata":{"id":"fRfYeB2AWGIr","colab_type":"code","colab":{}},"source":["#Define config parameters\n","encoder_embedding_size = 50\n","decoder_embedding_size = 50\n","rnn_units = 256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71ukvjyeWGIt","colab_type":"text"},"source":["#### Build Encoder"]},{"cell_type":"code","metadata":{"id":"2SHW_YvkWGIw","colab_type":"code","outputId":"fadeac63-81c6-409b-f859-98c4c0093836","executionInfo":{"status":"ok","timestamp":1578827758213,"user_tz":-330,"elapsed":892,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["#Input Layer\n","encoder_inputs = tf.keras.layers.Input(shape=(None,))\n","\n","#Embedding layer\n","encoder_embedding = tf.keras.layers.Embedding(encoder_vocab_size+1, encoder_embedding_size)\n","\n","#Get embedding layer output by feeding inputs\n","encoder_embedding_output = encoder_embedding(encoder_inputs)\n","\n","#---Following code has been commented out for Attention-------\n","#LSTM Layer and its output\n","#x, state_h, state_c = tf.keras.layers.LSTM(rnn_units,return_state=True)(encoder_embedding_output)\n","\n","#Build a list to feed Decoder\n","#encoder_states = [state_h, state_c]"],"execution_count":26,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YbfLAFglWGIx","colab_type":"text"},"source":["#### Build Encoder - Get all hidden states"]},{"cell_type":"code","metadata":{"id":"EnZJnNSbWGIz","colab_type":"code","colab":{}},"source":["#Create LSTM Layer and get All hidden states, last hidden and cell state\n","encoder_lstm = tf.keras.layers.LSTM(rnn_units,return_state=True, return_sequences=True)\n","\n","#Get 3 outputs of LSTM Layer\n","encoder_all_h_states, state_h, state_c = encoder_lstm(encoder_embedding_output)\n","\n","#Build a list to feed Decoder\n","encoder_states = [state_h, state_c]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAIuXWK5WGI1","colab_type":"text"},"source":["#### Build Decoder"]},{"cell_type":"code","metadata":{"id":"uYY-zwUSWGI2","colab_type":"code","colab":{}},"source":["#Decode input - padded Target sentences\n","decoder_inputs = tf.keras.layers.Input(shape=(None,))\n","\n","#Decoder Embedding layer\n","decoder_embedding = tf.keras.layers.Embedding(decoder_vocab_size + 1, decoder_embedding_size)\n","\n","#Embedding layer output\n","decoder_embedding_output = decoder_embedding(decoder_inputs)\n","\n","#Decoder RNN\n","decoder_rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n","\n","#Decoder RNN Output, State initialization from Encoder states\n","#Output will be all hidden sequences, last 'h' state and last 'c' state\n","decoder_all_h_states,_,_ = decoder_rnn(decoder_embedding_output, \n","                                       initial_state=encoder_states)\n","\n","#---Following code has been commented out for Attention-------\n","#Output Layer\n","#decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, activation='softmax')\n","\n","#Output of Dense layer\n","#decoder_outputs = decoder_dense(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwtwXesXWGI4","colab_type":"text"},"source":["#### Build Decoder...Alignment Matrix"]},{"cell_type":"code","metadata":{"id":"mOJtf8kMWGI5","colab_type":"code","colab":{}},"source":["#1. Dot Product between Decoder_all_h_states and encoder_all_h_states\n","#2. Apply softmax to get Alignment matrix\n","\n","#Dimensions details\n","#decoder_all_states = batch_size x max_decoder_length x rnn_units\n","#encoder_all_states = batch_size x max_encoder_length x rnn_units\n","#score = batch_size x max_decoder_length x max_encoder_length\n","#alignment matrix = batch_size x max_decoder_length x max_encoder_length\n","\n","score = tf.keras.layers.dot([decoder_all_h_states, encoder_all_h_states], axes=2)\n","alignment_matrix = tf.keras.layers.Activation('softmax')(score)\n","\n","#Try general and concat approaches to alignment matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNzzFkm4W0N8","colab_type":"code","outputId":"d590873d-ad8d-44b9-bffc-3893238ffce0","executionInfo":{"status":"ok","timestamp":1578828019812,"user_tz":-330,"elapsed":836,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["alignment_matrix"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'activation/truediv:0' shape=(?, ?, ?) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"cYFfIGyvWGI6","colab_type":"text"},"source":["#### Build Decoder...Context Vector"]},{"cell_type":"code","metadata":{"id":"h4ICneq6WGI6","colab_type":"code","colab":{}},"source":["#Weighted sum of multiplication of Alignment matrix and encoder states\n","#Dimension of context_vector =  batch_size x max_decoder_length x rnn_units\n","\n","context_vector = tf.keras.layers.dot([alignment_matrix, encoder_all_h_states], axes=[2,1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3EtN7Ho9fHy","colab_type":"code","outputId":"c93f427d-7a51-48f6-bb39-28ef00e90d9c","executionInfo":{"status":"ok","timestamp":1578828066301,"user_tz":-330,"elapsed":839,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["context_vector"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'dot_1/MatMul:0' shape=(?, ?, 256) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"4GOKQuJiWGI8","colab_type":"text"},"source":["#### Build Decoder...Attention Vector"]},{"cell_type":"code","metadata":{"id":"L9aJaK5LWGI8","colab_type":"code","colab":{}},"source":["#Concatenate context vector and decoder_all_h_states\n","#context_decoder_hidden = batch_size x max_decoder_length x rnn_units\n","#attention_vector = batch_size x max_decoder_length x 128\n","\n","context_decoder_hidden = tf.keras.layers.concatenate([context_vector, \n","                                                      decoder_all_h_states])\n","\n","attention_dense_layer = tf.keras.layers.Dense(128, use_bias=False, \n","                                              activation='tanh')\n","\n","attention_vector = attention_dense_layer(context_decoder_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1nE-nHNfqix","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"68be9117-fe28-4e49-b25b-3f6babbe3a75","executionInfo":{"status":"ok","timestamp":1578828215344,"user_tz":-330,"elapsed":870,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}}},"source":["attention_vector"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'dense/Tanh:0' shape=(?, ?, 128) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"yvcsEikBWGI-","colab_type":"text"},"source":["#### Build Decoder...Output layer"]},{"cell_type":"code","metadata":{"id":"GinU4PH5WGI_","colab_type":"code","colab":{}},"source":["#Output layer\n","decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, activation='softmax')\n","\n","#With attention input will be attention_vector and not decoder_all_h_states\n","decoder_outputs = decoder_dense(attention_vector)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__FmqX3qWGJL","colab_type":"text"},"source":["### Build Model using both Encoder and Decoder"]},{"cell_type":"code","metadata":{"id":"IbtWE0_JWGJL","colab_type":"code","colab":{}},"source":["model = tf.keras.models.Model([encoder_inputs, decoder_inputs], #2 Inputs to the model\n","                              decoder_outputs) #Output of the model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WG4pyK_jWGJO","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', \n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHhrDTBZWGJQ","colab_type":"text"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"81ciKSvCWGJQ","colab_type":"code","outputId":"52d3d3f0-bb34-486a-a038-8915eda905d2","executionInfo":{"status":"ok","timestamp":1578828937322,"user_tz":-330,"elapsed":654014,"user":{"displayName":"Rajeev Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBC_Jt2baS4Hv7JJvLmgAJFGZpvIs0sh5ggT7ZM9hw=s64","userId":"10567937244174773728"}},"colab":{"base_uri":"https://localhost:8080/","height":990}},"source":["model.fit([encoder_input_data, decoder_input_data], decoder_target_one_hot,\n","          batch_size=64,\n","          epochs=25,\n","          validation_split=0.2)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Train on 2223 samples, validate on 556 samples\n","Epoch 1/25\n","2223/2223 [==============================] - 28s 12ms/sample - loss: 4.3687 - acc: 0.7177 - val_loss: 3.3492 - val_acc: 0.5939\n","Epoch 2/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 2.1416 - acc: 0.7391 - val_loss: 3.3560 - val_acc: 0.5939\n","Epoch 3/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 2.1055 - acc: 0.7391 - val_loss: 3.3382 - val_acc: 0.5939\n","Epoch 4/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 2.0448 - acc: 0.7391 - val_loss: 3.1883 - val_acc: 0.5939\n","Epoch 5/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.8299 - acc: 0.7391 - val_loss: 2.9404 - val_acc: 0.5939\n","Epoch 6/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.6557 - acc: 0.7405 - val_loss: 2.7506 - val_acc: 0.5968\n","Epoch 7/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.5274 - acc: 0.7635 - val_loss: 2.6247 - val_acc: 0.6250\n","Epoch 8/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.4476 - acc: 0.7805 - val_loss: 2.5592 - val_acc: 0.6349\n","Epoch 9/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.3976 - acc: 0.7828 - val_loss: 2.5542 - val_acc: 0.6384\n","Epoch 10/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.3641 - acc: 0.7860 - val_loss: 2.4792 - val_acc: 0.6410\n","Epoch 11/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.3366 - acc: 0.7899 - val_loss: 2.4811 - val_acc: 0.6409\n","Epoch 12/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.3120 - acc: 0.7921 - val_loss: 2.4763 - val_acc: 0.6441\n","Epoch 13/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2924 - acc: 0.7929 - val_loss: 2.5034 - val_acc: 0.6429\n","Epoch 14/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2753 - acc: 0.7939 - val_loss: 2.4898 - val_acc: 0.6434\n","Epoch 15/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2600 - acc: 0.7943 - val_loss: 2.4903 - val_acc: 0.6450\n","Epoch 16/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2454 - acc: 0.7951 - val_loss: 2.4668 - val_acc: 0.6455\n","Epoch 17/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2312 - acc: 0.7958 - val_loss: 2.5096 - val_acc: 0.6438\n","Epoch 18/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2184 - acc: 0.7966 - val_loss: 2.4946 - val_acc: 0.6459\n","Epoch 19/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.2041 - acc: 0.7975 - val_loss: 2.5130 - val_acc: 0.6450\n","Epoch 20/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.1903 - acc: 0.7979 - val_loss: 2.4790 - val_acc: 0.6473\n","Epoch 21/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.1770 - acc: 0.7983 - val_loss: 2.5378 - val_acc: 0.6472\n","Epoch 22/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.1627 - acc: 0.7997 - val_loss: 2.5120 - val_acc: 0.6491\n","Epoch 23/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.1464 - acc: 0.8016 - val_loss: 2.5273 - val_acc: 0.6485\n","Epoch 24/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.1300 - acc: 0.8035 - val_loss: 2.5007 - val_acc: 0.6506\n","Epoch 25/25\n","2223/2223 [==============================] - 26s 12ms/sample - loss: 1.1144 - acc: 0.8048 - val_loss: 2.5640 - val_acc: 0.6477\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f33085de9e8>"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"0gO-p-OaWGJT","colab_type":"text"},"source":["### Save the model for later reuse"]},{"cell_type":"code","metadata":{"id":"GUJcwtNyWGJU","colab_type":"code","colab":{}},"source":["#model.save('models/seq2seq_training_translation_attention.hd5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjuM6UkoWGJW","colab_type":"code","colab":{}},"source":["model = tf.keras.models.load_model('models/seq2seq_training_translation_attention.hd5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8k64d1HWGJY","colab_type":"text"},"source":["# Building Model for Prediction"]},{"cell_type":"markdown","metadata":{"id":"_w__FLXCWGJZ","colab_type":"text"},"source":["### Build the Encoder Model to predict Encoder States"]},{"cell_type":"code","metadata":{"id":"x9sSoCtMWGJZ","colab_type":"code","colab":{}},"source":["encoder_model = tf.keras.models.Model(inputs=encoder_inputs, #Padded input sequences\n","                                      outputs=[encoder_all_h_states] + #Hidden states at all time steps\n","                                      encoder_states) #Hidden state and Cell state at last time step"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RScHHgr5WGJb","colab_type":"text"},"source":["### Build the Decoder Model \n","<p/>\n","\n","<ol><li>Define Input for both 'h' state and 'c' state initialization </li>\n","    <li><font color=\"blue\">Define Input for all encoder states - Attention Layer </font></li>\n","<li>Get Decoder RNN outputs along with h and c state</li>\n","<li><font color=\"blue\">Build Attention Layer</font></li>\n","<li><font color=\"blue\">Get Decoder Dense layer output using Attention vector</font></li>\n","    <li><font color=\"blue\">Build Model</font></li></ol>"]},{"cell_type":"markdown","metadata":{"id":"fZGRM_UQWGJc","colab_type":"text"},"source":["##### Step 1 - Define Input for both 'h' state and 'c' state initialization"]},{"cell_type":"code","metadata":{"id":"Wq4XvgTAWGJc","colab_type":"code","colab":{}},"source":["#Hidden state input\n","decoder_state_input_h = tf.keras.layers.Input(shape=(rnn_units,))\n","\n","#Cell state input\n","decoder_state_input_c = tf.keras.layers.Input(shape=(rnn_units,))\n","\n","#Putting it together\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4rg9wJMWGJe","colab_type":"text"},"source":["##### Step 2 - Define Input encoder states - Attention Layer"]},{"cell_type":"code","metadata":{"id":"SYozwBH2WGJe","colab_type":"code","colab":{}},"source":["encoder_outputs = tf.keras.layers.Input(shape=(max_encoder_seq_length, rnn_units,))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cu7AJ7teWGJg","colab_type":"text"},"source":["##### Step 3 - Get Decoder RNN outputs along with h and c state"]},{"cell_type":"code","metadata":{"id":"TPbxVkdoWGJh","colab_type":"code","colab":{}},"source":["#Get Embedding layer output\n","x = decoder_embedding(decoder_inputs)\n","\n","#We will use the layer which we trained earlier\n","rnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)\n","\n","#Why do we need this?\n","decoder_states = [state_h, state_c]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YnDK-mtkWGJk","colab_type":"text"},"source":["##### Step 4 - Build Attention Layer"]},{"cell_type":"code","metadata":{"id":"9RFkceyHWGJk","colab_type":"code","colab":{}},"source":["#Alignment score\n","p_score = tf.keras.layers.dot([rnn_outputs, encoder_outputs], axes=2)\n","\n","#Perform softmax to get Alignment matrix\n","p_alignment_matrix = tf.keras.layers.Activation('softmax')(p_score)\n","\n","#Context Vector\n","p_context_vector = tf.keras.layers.dot([p_alignment_matrix, encoder_outputs], axes=[2,1])\n","\n","#Build Attention Vector\n","# 1. Caoncatenate both context vector and decoder outputs\n","# 2. Feed it to the Dense layer \n","p_context_decoder_hidden = tf.keras.layers.concatenate([p_context_vector, rnn_outputs])\n","p_attention_vector = attention_dense_layer(p_context_decoder_hidden)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLw9G3KkWGJl","colab_type":"code","outputId":"c6708e95-d1bd-42f0-c5f1-4b0ebca1847a","colab":{}},"source":["p_alignment_matrix"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'activation_1/truediv:0' shape=(?, ?, 22) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"fUVTE025WGJn","colab_type":"text"},"source":["##### Step 5 - Get Decoder Dense layer output"]},{"cell_type":"code","metadata":{"id":"o60fmWbJWGJo","colab_type":"code","colab":{}},"source":["decoder_outputs = decoder_dense(p_attention_vector)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oE7o-lOfWGJ3","colab_type":"text"},"source":["##### Step 6 - Build Decoder Model"]},{"cell_type":"code","metadata":{"id":"545vI4cXWGJ4","colab_type":"code","colab":{}},"source":["#3 Inputs - Word, h/c state and all hidden states from encoder\n","#3 Outputs - predicted word, h and c state values for next run and alignment matrix for visualization\n","\n","decoder_model = tf.keras.models.Model([decoder_inputs] +  #Start sequence and then word\n","                                      decoder_states_inputs + #h and c state value for initialization\n","                                      [encoder_outputs],  #Encoder all hidden states for Attention layer\n","                                      [decoder_outputs] + #Model word prediction\n","                                      decoder_states +   #h and c states for next run\n","                                      [p_alignment_matrix]) #for Alignment matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7VEzNGyWGJ5","colab_type":"text"},"source":["# Predicting output from Seq2Seq model"]},{"cell_type":"markdown","metadata":{"id":"mcg5mBcYWGJ6","colab_type":"text"},"source":["##### Build a prediction function"]},{"cell_type":"code","metadata":{"id":"J1VoPsIkWGJ6","colab_type":"code","colab":{}},"source":["def decode_sentence(input_sequence):\n","    \n","    #Get the encoder state values\n","    encoder_output =  encoder_model.predict(input_sequence)\n","    decoder_initial_states_value = encoder_output[1:]    \n","    encoded_seqs = encoder_output[0]\n","    \n","    #Build a sequence with '<start>' - starting sequence for Decoder\n","    target_seq = np.zeros((1,1))    \n","    target_seq[0][0] = decoder_t.word_index['<start>']\n","    \n","    #flag to check if prediction should be stopped\n","    stop_loop = False\n","    \n","    #Initialize predicted sentence\n","    predicted_sentence = ''\n","    \n","    #start the loop\n","    while not stop_loop:\n","        \n","        #Decoder model with 3 inputs\n","        predicted_outputs, h, c, a_matrix = decoder_model.predict([target_seq] + \n","                                                                  decoder_initial_states_value +\n","                                                                  [encoded_seqs])\n","        \n","        #Get the predicted word index with highest probability\n","        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n","        \n","        #Get the predicted word from predicter index\n","        if (predicted_output == 0):\n","            predicted_word = ' '\n","        else:\n","            predicted_word = int_to_word_decoder[predicted_output]\n","        \n","        #Check if prediction should stop\n","        if(predicted_word == '<end>' or len(predicted_sentence) > max_decoder_seq_length):\n","            \n","            stop_loop = True\n","            continue\n","                    \n","        #Updated predicted sentence\n","        if (len(predicted_sentence) == 0):\n","            predicted_sentence = predicted_word\n","        else:\n","            predicted_sentence = predicted_sentence + ' ' + predicted_word\n","            \n","        #Update target_seq to be the predicted word index\n","        target_seq[0][0] = predicted_output\n","        \n","        #Update initial states value for decoder\n","        decoder_initial_states_value = [h,c]\n","        \n","        #print(a_matrix)\n","    \n","    return predicted_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CuQcVTVZWGJ8","colab_type":"text"},"source":["##### Call Prediction function on a random sentence"]},{"cell_type":"code","metadata":{"id":"jkJznTF8WGJ9","colab_type":"code","outputId":"0eb5c8f5-f122-4c71-d2c4-0b990bca8b60","colab":{}},"source":["#Generate a random number\n","start_num = np.random.randint(0, high=len(encoder_text) - 10)\n","\n","#Predict model output for 5 sentences\n","for i in range(start_num, start_num + 1):\n","    input_seq = encoder_input_data[i : i+1]\n","    #print(input_seq)\n","    predicted_sentence = decode_sentence(input_seq)\n","    print('--------')\n","    print ('Input sentence: ', encoder_text[i])\n","    print ('Predicted sentence: ', predicted_sentence )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------\n","Input sentence:  I laughed.\n","Predicted sentence:  वह अपने लिए बहुत बहुत नहीं है।\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YS-RjZPlWGJ-","colab_type":"text"},"source":["##### Save encoder and decoder model"]},{"cell_type":"code","metadata":{"id":"2l-qAT-bWGJ-","colab_type":"code","colab":{}},"source":["#Compile models to avoid error\n","encoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","decoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","\n","#Save the models\n","encoder_model.save('models/seq2seq_encoder_eng_hin.hd5')  #Encoder model\n","decoder_model.save('models/seq2seq_decoder_eng_hin.hd5')  #Decoder model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oh0okCv0WGKA","colab_type":"text"},"source":["##### Save encoder and decoder tokenizers"]},{"cell_type":"code","metadata":{"id":"Qwfb-v7OWGKB","colab_type":"code","colab":{}},"source":["import pickle\n","\n","pickle.dump(encoder_t,open('models/encoder_tokenizer_eng','wb'))\n","pickle.dump(decoder_t,open('models/decoder_tokenizer_hin','wb'))"],"execution_count":0,"outputs":[]}]}