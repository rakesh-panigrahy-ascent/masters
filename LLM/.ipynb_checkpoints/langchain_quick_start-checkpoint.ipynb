{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e81031b8-ee0e-4060-b40f-733f9deb0818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_api_key='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'\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d0b046-7865-4c8a-a10b-388c77ae5686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can help with testing in the following ways:\\n\\n1. Test Coverage: Langsmith can analyze the codebase and provide insights on the coverage of tests. It can identify areas with low coverage, helping developers prioritize their testing efforts.\\n\\n2. Test Generation: Langsmith can automatically generate test cases based on the code's structure and specifications. It can explore different execution paths and edge cases, reducing the manual effort required for test creation.\\n\\n3. Test Optimization: Langsmith can optimize existing test suites by identifying redundant or unnecessary test cases. This helps in reducing the execution time and improving the overall efficiency of the testing process.\\n\\n4. Test Oracles: Langsmith can assist in creating oracles, which are mechanisms to determine the correctness of test results. It can help in automatically comparing expected and actual outputs, simplifying the verification process.\\n\\n5. Bug Detection: Langsmith can identify potential bugs or vulnerabilities in the codebase by performing static analysis. It can detect common coding errors, security vulnerabilities, and potential performance issues, making it easier to catch bugs early in the development lifecycle.\\n\\n6. Test Maintenance: Langsmith can aid in maintaining test suites by automatically updating test cases when the codebase changes. It can identify the impact of code modifications and adapt the tests accordingly, ensuring that the test suite remains up-to-date.\\n\\nOverall, Langsmith can be a valuable tool in improving the effectiveness and efficiency of testing efforts, helping developers build more reliable and robust software.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b453820a-a14d-460a-bd01-95b859327034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e33dec-2a81-437b-94ed-f5ddb19dcedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Chain\n",
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816e21ee-e68a-4f66-907e-f0c4e148a2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Langsmith can be a valuable tool for testing purposes in several ways:\\n\\n1. Test Automation: Langsmith can generate realistic and diverse test data for automated testing. By utilizing its language generation capabilities, it can produce various inputs that cover different scenarios and edge cases. This helps ensure comprehensive test coverage and increases the likelihood of identifying bugs and issues.\\n\\n2. Test Case Generation: Langsmith can assist in generating test cases by automatically creating test scripts or scenarios based on predefined criteria. It can generate test inputs, expected outputs, and even simulate user interactions with the system being tested.\\n\\n3. Test Data Management: Langsmith can help with managing test data by generating realistic and representative datasets. This is particularly useful when there is a need for large volumes of data for performance or stress testing. Langsmith can generate synthetic data that closely mimics real-world scenarios, allowing testers to simulate different conditions and evaluate system behavior.\\n\\n4. Exploratory Testing: Langsmith can support exploratory testing efforts by providing instant, on-demand content generation. Testers can interact with Langsmith to quickly generate sample data, text, or any other content required for their exploratory testing sessions. This saves time and effort in creating or searching for suitable test inputs manually.\\n\\n5. Documentation and Training: Langsmith can be used to create sample documentation or training materials for testing purposes. It can generate realistic user scenarios, sample inputs, and expected outputs to help testers understand the software or system being tested. This can be especially helpful for onboarding new testers or conducting training sessions.\\n\\nOverall, Langsmith's language generation capabilities can enhance different aspects of testing, including test automation, test case generation, test data management, exploratory testing, and documentation/training. Its versatility makes it a valuable tool for testers looking to optimize their testing efforts.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e895efa6-3fd5-49f3-b3e6-db782e342f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's add a simple output parser to convert the chat message to a string\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76a76a6-021f-413d-b2b7-3c5221557c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92573c11-d76f-4400-9b38-f4232dd19a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can be a valuable tool for testing software applications in several ways:\\n\\n1. Test Case Generation: Langsmith uses artificial intelligence and natural language processing to automatically generate test cases based on the provided requirements or specifications. This can significantly reduce the time and effort required for manual test case creation.\\n\\n2. Test Data Generation: Langsmith can generate realistic and diverse test data based on the input requirements. This ensures that the application is tested with a variety of data inputs, helping to uncover potential bugs or issues.\\n\\n3. Test Scenario Analysis: Langsmith can analyze test scenarios and identify potential edge cases or boundary conditions that need to be tested. This helps in ensuring comprehensive test coverage and identifying areas of the application that may require additional testing.\\n\\n4. Test Script Automation: Langsmith can generate test scripts that can be directly executed by testing tools or frameworks. This helps in automating the testing process and reducing the effort required for manual testing.\\n\\n5. Test Result Analysis: Langsmith can analyze test results and provide insights into the root cause of failures or issues. This helps in identifying the areas of the application that require further investigation or debugging.\\n\\n6. Test Documentation: Langsmith can assist in generating test documentation, including test plans, test cases, and test reports. This ensures that the testing process is well-documented and helps in maintaining a comprehensive record of the testing activities.\\n\\nOverall, Langsmith can enhance the testing process by automating test case generation, providing intelligent test data, analyzing test scenarios, automating test scripts, analyzing test results, and generating test documentation.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986fa74a-84f3-4a3c-bea6-aa11ea427e91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "n order to properly answer the original question (\"how can langsmith help with testing?\"), \n",
    "we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful \n",
    "when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the \n",
    "most relevant pieces and pass those in.\n",
    "\n",
    "In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. \n",
    "A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate \n",
    "a vector store and use that as a retriever. For more information on vectorstores, see this documentation.\n",
    "'''\n",
    "\n",
    "#First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. \n",
    "#This requires installing BeautifulSoup:\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd65c326-16ea-4ab7-99e6-8dcff18551f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nLangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith\\n\\n\\n\\n\\n\\nSkip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what\\'s going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being made? In what order? What are the inputs and outputs of each call?LangSmith\\'s built-in tracing feature offers a visualization to clarify these sequences. This tool is invaluable for understanding intricate and lengthy chains and agents. For chains, it can shed light on the sequence of calls and how they interact. For agents, where the sequence of calls is non-deterministic, it helps visualize the specific sequence for a given run -- something that is impossible to know ahead of time.Why did my chain take much longer than expected?â€‹If a chain takes longer than expected, you need to identify the cause. By tracking the latency of each step, LangSmith lets you identify and possibly eliminate the slowest components.How many tokens were used?â€‹Building and prototyping LLM applications can be expensive. LangSmith tracks the total token usage for a chain and the token usage of each step. This makes it easy to identify potentially costly parts of the chain.Collaborative debuggingâ€‹In the past, sharing a faulty chain with a colleague for debugging was challenging when performed locally. With LangSmith, we\\'ve added a â€œShareâ€� button that makes the chain and LLM runs accessible to anyone with the shared link.Collecting examplesâ€‹Most of the time we go to debug, it\\'s because something bad or unexpected outcome has happened in our application. These failures are valuable data points! By identifying how our chain can fail and monitoring these failures, we can test future chain versions against these known issues.Why is this so impactful? When building LLM applications, itâ€™s often common to start without a dataset of any kind. This is part of the power of LLMs! They are amazing zero-shot learners, making it possible to get started as easily as possible. But this can also be a curse -- as you adjust the prompt, you\\'re wandering blind. You donâ€™t have any examples to benchmark your changes against.LangSmith addresses this problem by including an â€œAdd to Datasetâ€� button for each run, making it easy to add the input/output examples a chosen dataset. You can edit the example before adding them to the dataset to include the expected result, which is particularly useful for bad examples.This feature is available at every step of a nested chain, enabling you to add examples for an end-to-end chain, an intermediary chain (such as a LLM Chain), or simply the LLM or Chat Model.End-to-end chain examples are excellent for testing the overall flow, while single, modular LLM Chain or LLM/Chat Model examples can be beneficial for testing the simplest and most directly modifiable components.Testing & evaluationâ€‹Initially, we do most of our evaluation manually and ad hoc. We pass in different\\ninputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.\\nYou can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We\\'re eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d2e726-b767-49e8-9f49-93f517d95e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Next, we need to index it into a vectorstore. This requires a few components, namely an embedding model \n",
    "and a vectorstore.\n",
    "\n",
    "For embedding models, we once again provide examples for accessing via OpenAI or via local models.\n",
    "'''\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5b9802-0f7c-48ae-9c20-f1d945daecb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp311-cp311-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m810.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, FAISS, for simplicity's sake.\n",
    "\n",
    "First we need to install the required packages for that:\n",
    "'''\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c583dd6-7fce-4ba2-970b-527137a04588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4cfd786-1a9c-4b45-b17a-b507ef366b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now that we have this data indexed in a vectorstore, we will create a retrieval chain. \n",
    "This chain will take an incoming question, look up relevant documents, then pass those documents along with \n",
    "the original question into an LLM and ask it to answer the original question.\n",
    "\n",
    "First, let's set up the chain that takes a question and the retrieved documents and generates an answer.\n",
    "'''\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab897932-ad93-43da-9fbb-de9fa4b612c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith can help with testing by allowing users to visualize test results.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we wanted to, we could run this ourselves by passing in documents directly:\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ffeea10-beb1-42ff-88c3-96991be266e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "However, we want the documents to first come from the retriever we just set up. That way, for a given \n",
    "question we can use the retriever to dynamically select the most relevant documents and pass those in\n",
    "'''\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "084abd79-4c62-4ee2-925d-467d542ff637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith can help with testing by providing the ability to run chains over data points and visualize the outputs. It simplifies dataset uploading and allows users to review and assign feedback to the runs. LangSmith also offers evaluators that can be specified during a test run to evaluate the results. Additionally, LangSmith facilitates human evaluation through annotation queues, allowing reviewers to manually review and annotate runs based on criteria like model type or automatic evaluation scores.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We can now invoke this chain. This returns a dictionary - the response from the LLM is in the \n",
    "answer key\n",
    "'''\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e483b92b-2b17-41df-b6e4-a3e951d1f3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The chain we've created so far can only answer single questions. One of the main types of LLM applications \n",
    "that people are building are chat bots. So how do we turn this chain into one that can answer follow up \n",
    "questions?\n",
    "\n",
    "We can still use the create_retrieval_chain function, but we need to change two things:\n",
    "\n",
    "The retrieval method should now not just work on the most recent input, but rather should take the whole \n",
    "history into account.\n",
    "\n",
    "The final LLM chain should likewise take the whole history into account\n",
    "Updating Retrieval\n",
    "\n",
    "In order to update retrieval, we will create a new chain. This chain will take in the most recent \n",
    "input (input) and the conversation history (chat_history) and use an LLM to generate a search query.\n",
    "'''\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8406469-1bfb-485b-941b-1063cbd14292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       " Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We can test this out by passing in an instance where the user is asking a follow up question.\n",
    "'''\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b0bee1e-ea64-476a-9de7-fdad597fe2bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "You should see that this returns documents about testing in LangSmith. This is because the LLM generated \n",
    "a new query, combining the chat history with the follow up question.\n",
    "\n",
    "Now that we have this new retriever, we can create a new chain to continue the conversation with these \n",
    "retrieved documents in mind.\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8cb227f-79f4-4a6f-863a-090e08147aeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"Skip to main contentğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain have been building and using LangSmith with the goal of bridging this gap. This is our tactical user guide to outline effective ways to use LangSmith and maximize its benefits.On by defaultâ€‹At LangChain, all of us have LangSmithâ€™s tracing running in the background by default. On the Python side, this is achieved by setting environment variables, which we establish whenever we launch a virtual environment or open our bash shell and leave them set. The same principle applies to most JavaScript environments through process.env1.The benefit here is that all calls to LLMs, chains, agents, tools, and retrievers are logged to LangSmith. Around 90% of the time we donâ€™t even look at the traces, but the 10% of the time that we doâ€¦ itâ€™s so helpful. We can use LangSmith to debug:An unexpected end resultWhy an agent is loopingWhy a chain was slower than expectedHow many tokens an agent usedDebuggingâ€‹Debugging LLMs, chains, and agents can be tough. LangSmith helps solve the following pain points:What was the exact input to the LLM?â€‹LLM calls are often tricky and non-deterministic. The inputs/outputs may seem straightforward, given they are technically string â†’ string (or chat messages â†’ chat message), but this can be misleading as the input string is usually constructed from a combination of user input and auxiliary functions.Most inputs to an LLM call are a combination of some type of fixed template along with input variables. These input variables could come directly from user input or from an auxiliary function (like retrieval). By the time these input variables go into the LLM they will have been converted to a string format, but often times they are not naturally represented as a string (they could be a list, or a Document object). Therefore, it is important to have visibility into what exactly the final string going into the LLM is. This has helped us debug bugs in formatting logic, unexpected transformations to user input, and straight up missing user input.To a much lesser extent, this is also true of the output of an LLM. Oftentimes the output of an LLM is technically a string but that string may contain some structure (json, yaml) that is intended to be parsed into a structured representation. Understanding what the exact output is can help determine if there may be a need for different parsing.LangSmith provides a straightforward visualization of the exact inputs/outputs to all LLM calls, so you can easily understand them.If I edit the prompt, how does that affect the output?â€‹So you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?â€‹In complicated chains and agents, it can often be hard to understand what is going on under the hood. What\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content=\"You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoringâ€‹After all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be assigned string tags or key-value metadata, allowing you to attach correlation ids or AB test variants, and filter runs accordingly.Weâ€™ve also made it possible to associate feedback programmatically with runs. This means that if your application has a thumbs up/down button on it, you can use that to log feedback back to LangSmith. This can be used to track performance over time and pinpoint under performing data points, which you can subsequently add to a dataset for future testing â€” mirroring the debug mode approach.Weâ€™ve provided several examples in the LangSmith documentation for extracting insights from logged runs. In addition to guiding you on performing this task yourself, we also provide examples of integrating with third parties for this purpose. We're eager to expand this area in the coming months! If you have ideas for either -- an open-source way to evaluate, or are building a company that wants to do analytics over these runs, please reach out.Exporting datasetsâ€‹LangSmith makes it easy to curate datasets. However, these arenâ€™t just useful inside LangSmith; they can be exported for use in other contexts. Notable applications include exporting for use in OpenAI Evals or fine-tuning, such as with FireworksAI.To set up tracing in Deno, web browsers, or other runtime environments without access to the environment, check out the FAQs.â†©PreviousLangSmithNextTracingOn by defaultDebuggingWhat was the exact input to the LLM?If I edit the prompt, how does that affect the output?What is the exact sequence of events?Why did my chain take much longer than expected?How many tokens were used?Collaborative debuggingCollecting examplesTesting & evaluationHuman EvaluationMonitoringExporting datasetsCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogCopyright Â© 2024 LangChain, Inc.\", metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'}),\n",
       "  Document(page_content='inputs, and see what happens. At some point though, our application is performing\\nwell and we want to be more rigorous about testing changes. We can use a dataset\\nthat weâ€™ve constructed along the way (see above). Alternatively, we could spend some\\ntime constructing a small dataset by hand. For these situations, LangSmith simplifies\\ndataset uploading.Once we have a dataset, how can we use it to test changes to a prompt or chain? The most basic approach is to run the chain over the data points and visualize the outputs. Despite technological advancements, there still is no substitute for looking at outputs by eye. Currently, running the chain over the data points needs to be done client-side. The LangSmith client makes it easy to pull down a dataset and then run a chain over them, logging the results to a new project associated with the dataset. From there, you can review them. We\\'ve made it easy to assign feedback to runs and mark them as correct or incorrect directly in the web app, displaying aggregate statistics for each test project.We also make it easier to evaluate these runs. To that end, we\\'ve added a set of evaluators to the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. If weâ€™re being honest, most of these evaluators aren\\'t perfect. We would not recommend that you trust them blindly. However, we do think they are useful for guiding your eye to examples you should look at. This becomes especially valuable as the number of data points increases and it becomes infeasible to look at each one manually.Human Evaluationâ€‹Automatic evaluation metrics are helpful for getting consistent and scalable information about model behavior;\\nhowever, there is still no complete substitute for human review to get the utmost quality and reliability from your application.\\nLangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like creativity or humorTo sample and validate a subset of runs that were auto-evaluated, ensuring the automatic metrics are still reliable and capturing the right informationAll the annotations made using the queue are assigned as \"feedback\" to the source runs, so you can easily filter and analyze them later.', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | ğŸ¦œï¸�ğŸ›\\xa0ï¸� LangSmith', 'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en'})],\n",
       " 'answer': 'LangSmith can help test your LLM applications in several ways:\\n\\n1. Dataset Testing: LangSmith simplifies dataset uploading, allowing you to test changes to prompts or chains using a dataset. You can run the chain over the data points and visualize the outputs. The LangSmith client makes it easy to pull down a dataset, run a chain over them, and log the results to a new project associated with the dataset. You can review the results, assign feedback to runs, and mark them as correct or incorrect in the web app.\\n\\n2. Evaluators: LangSmith provides a set of evaluators in the open-source LangChain library. These evaluators can be specified when initiating a test run and will evaluate the results once the test run completes. While these evaluators are not perfect, they can guide your attention to examples you should look at, especially when dealing with a large number of data points.\\n\\n3. Human Evaluation: LangSmith makes it easy to manually review and annotate runs through annotation queues. You can select runs based on criteria like model type or automatic evaluation scores and queue them up for human review. As a reviewer, you can quickly step through the runs, view inputs, outputs, and existing tags, and add your own feedback. This is useful for assessing subjective qualities that automatic evaluators struggle with, such as creativity or humor, and validating the reliability of automatic metrics.\\n\\nBy using these testing features, you can ensure the quality and reliability of your LLM applications.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now test this out end-to-end:\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35364a6b-ece2-4a8c-a4d2-09243dbb1ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47519020-2db6-408b-8b27-7fde0ba2da09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Agent\n",
    "We've so far create examples of chains - where each step is known ahead of time. The final thing we will \n",
    "create is an agent - where the LLM decides what steps to take.\n",
    "\n",
    "NOTE: for this example we will only show how to create an agent using OpenAI models, as local models \n",
    "are not reliable enough yet.\n",
    "\n",
    "One of the first things to do when building an agent is to decide what tools it should have access to. \n",
    "For this example, we will give the agent access two tools:\n",
    "\n",
    "The retriever we just created. This will let it easily answer questions about LangSmith\n",
    "A search tool. This will let it easily answer questions that require up to date information.\n",
    "First, let's set up a tool for the retriever we just created:\n",
    "'''\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0530458d-d7d9-4926-8551-531e275dc1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = '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'\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e59ae327-a9a8-447f-800c-d1d3825bb6d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "921a9a20-40d5-4f71-82c2-fbcfbf76e281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can now create a list of the tools we want to work with:\n",
    "\n",
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aeb03c8f-8acf-4814-b33d-6c0e44422a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Obtaining dependency information for langchainhub from https://files.pythonhosted.org/packages/9c/99/4306b825b9462c824fd6d8dcb959309fb5e654c0d7b6473c5379388a82e9/langchainhub-0.1.14-py3-none-any.whl.metadata\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Obtaining dependency information for types-requests<3.0.0.0,>=2.31.0.2 from https://files.pythonhosted.org/packages/ae/29/3290a0d17865b9ec3d54fbb17faa265d9de7e856b6a3b52dfdf3507efd7c/types_requests-2.31.0.20240125-py3-none-any.whl.metadata\n",
      "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2023.11.17)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchainhub)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/88/75/311454fd3317aefe18415f04568edc20218453b709c63c58b9292c71be17/urllib3-2.2.0-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
      "Downloading types_requests-2.31.0.20240125-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.2.0-py3-none-any.whl (120 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, types-requests, langchainhub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.29.76 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchainhub-0.1.14 types-requests-2.31.0.20240125 urllib3-2.2.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - \n",
    "for a deeper dive into what exactly is going on, check out the Agent's Getting Started documentation\n",
    "\n",
    "Install langchain hub first\n",
    "'''\n",
    "\n",
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd00fa83-7aa3-408d-9311-634b61df8f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we can use it to get a predefined prompt\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=openai_api_key)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3238ea25-2053-4764-8413-51cb0e4871a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help with testing in several ways:\n",
      "\n",
      "1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\n",
      "\n",
      "2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\n",
      "\n",
      "3. Test Coverage Analysis: LangSmith can analyze the code coverage of the test cases to determine which parts of the software have been exercised during testing. This helps in identifying areas of the code that have not been adequately tested and may require additional test cases.\n",
      "\n",
      "4. Test Prioritization: LangSmith can prioritize the execution of test cases based on their likelihood of finding bugs or their impact on the system. This helps in optimizing the testing process by focusing on the most critical or high-risk areas of the software.\n",
      "\n",
      "5. Test Result Analysis: LangSmith can analyze the results of the test cases to identify patterns or trends that may indicate the presence of bugs or performance issues. It can also provide insights into the root causes of failures and suggest possible fixes or improvements.\n",
      "\n",
      "Overall, LangSmith can assist in automating and optimizing various aspects of the testing process, leading to more efficient and effective testing of software systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'how can langsmith help with testing?',\n",
       " 'output': 'LangSmith can help with testing in several ways:\\n\\n1. Test Case Generation: LangSmith can generate test cases automatically based on the specifications of the software being tested. It uses advanced techniques such as symbolic execution and constraint solving to generate test inputs that cover different execution paths and edge cases.\\n\\n2. Test Oracles: LangSmith can help in defining test oracles, which are the expected outputs or behaviors of the software under test. It can analyze the specifications and requirements of the software to automatically generate assertions or checks that can be used as test oracles.\\n\\n3. Test Coverage Analysis: LangSmith can analyze the code coverage of the test cases to determine which parts of the software have been exercised during testing. This helps in identifying areas of the code that have not been adequately tested and may require additional test cases.\\n\\n4. Test Prioritization: LangSmith can prioritize the execution of test cases based on their likelihood of finding bugs or their impact on the system. This helps in optimizing the testing process by focusing on the most critical or high-risk areas of the software.\\n\\n5. Test Result Analysis: LangSmith can analyze the results of the test cases to identify patterns or trends that may indicate the presence of bugs or performance issues. It can also provide insights into the root causes of failures and suggest possible fixes or improvements.\\n\\nOverall, LangSmith can assist in automating and optimizing various aspects of the testing process, leading to more efficient and effective testing of software systems.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:\n",
    "\n",
    "agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b872dd58-e821-42cc-a6cb-f023e4b55ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `tavily_search_results_json` with `{'query': 'weather in Diva Maharashtra'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'url': 'https://weather.com/en-IN/india/news/news/2024-01-29-mumbai-to-end-january-warmly-cold-to-return-in-february', 'content': \"Weather News Mumbai to End January on a Warmer Note; Cool and Pleasant Weather to Return Come February  Mumbai's Santacruz observatory readings indicate maximum temperatures settling around 35°C since the weekend,  coastal observatory noted daytime temperatures reaching 33°C on Sunday (Jan 28).  Even the minimum temperatures recorded in Santacruz and Colaba during Monday's early hours stood at 18.4°C and 21°C,But don't let the recent cold fool you; Mumbai has experienced a warm winter this year. On January 12, the financial capital recorded a maximum temperature of 35.7°C, marking its warmest ...\"}, {'url': 'https://www.onefivenine.com/india/villages/Thane/Thane/Diva', 'content': 'Jay Mata Di Society; Patil Construction Local Parks in Diva,Thane Laxmi Park Padle Gaon Bird Watching  Gurudwara Dafagr Jublipark Bilding N2 PVR Cinemas PVR Xperia Mall Palava Temples in Diva,Thane Masoba Dev Mandir  Masoba Dev Mandir Vitthal Mandir Gaon dev temple Om Sai Ganesh Mitra Mandal Mosques in Diva,Thane Raza Masjid  Bholenath Complex à¤¨à¤¾à¤¨à¤¾ à¤¹à¤¾à¤¡à¤µà¥ˆà¤¦à¥�à¤¯ Petrol Bunks in Diva,ThaneTemperature: 23.0 °C smoke Humidity: 31% Wind : 0 mt/sec towards N StationName : \"ThÄ\\\\u0081ne\" observed on 2 Hours Back Diva Weather Forecast for Next 5 days 24-01-2024'}, {'url': 'https://www.hindustantimes.com/education/competitive-exams/mah-cet-2024-last-date-to-apply-extended-for-mba-3-year-llb-other-courses-101706600320649.html', 'content': '2024 will be held on March 9 and March 10. The MAH-B.Ed (General & Special) & B.Ed ELCTCET-2024 exam will be conducted  been extended for MAH-B.Ed.M.Ed.(Three Year Integrated Course)- CET)-2024, MAH-M.Ed CET-2024, MAH-M.P.Ed. CET-2024,  The MAH-L.L.B.3 Yrs. CET 2024 exam will be conducted on March 12 and March 13. The MAH- MBA/MMS-CET 2024 will be held  will be conducted on March 4 to March 6. The MAH-MCA CET-2024 exam will be held on March 14.The last day to apply for the MAH MBA CET 2024 and other courses has been extended to February 6, and the registration period for the MAH LLB 3-year CET 2024 has been extended to February 10.'}]\u001b[0m\u001b[32;1m\u001b[1;3mI couldn't find the specific weather information for Diva, Maharashtra. However, I found some general weather news for Mumbai, which is close to Diva. According to the news, Mumbai has been experiencing warm temperatures with maximum temperatures settling around 35°C. The minimum temperatures recorded in Santacruz and Colaba were 18.4°C and 21°C respectively. Please note that this information may not be specific to Diva, Maharashtra.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is the weather in Diva Maharshtra?',\n",
       " 'output': \"I couldn't find the specific weather information for Diva, Maharashtra. However, I found some general weather news for Mumbai, which is close to Diva. According to the news, Mumbai has been experiencing warm temperatures with maximum temperatures settling around 35°C. The minimum temperatures recorded in Santacruz and Colaba were 18.4°C and 21°C respectively. Please note that this information may not be specific to Diva, Maharashtra.\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can ask it about the weather:\n",
    "\n",
    "agent_executor.invoke({\"input\": \"what is the weather in Diva Maharshtra?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af3359a1-2e9a-48fc-af36-4786d087b141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLangSmith can help test your LLM applications in the following ways:\n",
      "\n",
      "1. Personal Statement Review: LangSmith can provide feedback and suggestions on your personal statement to ensure it effectively communicates your motivations, experiences, and goals. This can help you make a strong impression on the admissions committee.\n",
      "\n",
      "2. Resume/CV Review: LangSmith can review your resume or CV and provide recommendations on how to highlight your relevant experiences, skills, and achievements. This can help you present a compelling case for your admission.\n",
      "\n",
      "3. Application Strategy: LangSmith can help you develop a strategic approach to your LLM applications, including selecting the right programs, organizing your application materials, and managing deadlines. This can help you maximize your chances of getting accepted to your desired programs.\n",
      "\n",
      "4. Interview Preparation: LangSmith can assist you in preparing for LLM interviews by conducting mock interviews and providing feedback on your performance. This can help you feel more confident and prepared during the actual interviews.\n",
      "\n",
      "5. Research on LLM Programs: LangSmith can provide information and insights on different LLM programs, including their curriculum, faculty, reputation, and career opportunities. This can help you make informed decisions when selecting the programs to apply to.\n",
      "\n",
      "Please note that while LangSmith can provide valuable guidance and support, ultimately, the success of your LLM applications depends on your own efforts and qualifications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'output': 'LangSmith can help test your LLM applications in the following ways:\\n\\n1. Personal Statement Review: LangSmith can provide feedback and suggestions on your personal statement to ensure it effectively communicates your motivations, experiences, and goals. This can help you make a strong impression on the admissions committee.\\n\\n2. Resume/CV Review: LangSmith can review your resume or CV and provide recommendations on how to highlight your relevant experiences, skills, and achievements. This can help you present a compelling case for your admission.\\n\\n3. Application Strategy: LangSmith can help you develop a strategic approach to your LLM applications, including selecting the right programs, organizing your application materials, and managing deadlines. This can help you maximize your chances of getting accepted to your desired programs.\\n\\n4. Interview Preparation: LangSmith can assist you in preparing for LLM interviews by conducting mock interviews and providing feedback on your performance. This can help you feel more confident and prepared during the actual interviews.\\n\\n5. Research on LLM Programs: LangSmith can provide information and insights on different LLM programs, including their curriculum, faculty, reputation, and career opportunities. This can help you make informed decisions when selecting the programs to apply to.\\n\\nPlease note that while LangSmith can provide valuable guidance and support, ultimately, the success of your LLM applications depends on your own efforts and qualifications.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can have conversations with it:\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "agent_executor.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7986675c-6cb6-4a5f-8587-fa44a87ca5a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langserve[all] in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (0.0.41)\n",
      "Requirement already satisfied: fastapi<1,>=0.90.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (0.109.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (0.23.0)\n",
      "Requirement already satisfied: httpx-sse>=0.3.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (0.4.0)\n",
      "Requirement already satisfied: langchain>=0.0.333 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (0.1.4)\n",
      "Requirement already satisfied: orjson>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (3.9.12)\n",
      "Requirement already satisfied: pydantic>=1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (1.10.8)\n",
      "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langserve[all]) (1.8.2)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (2023.11.17)\n",
      "Requirement already satisfied: sniffio in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (1.2.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (1.4.0)\n",
      "Requirement already satisfied: httpcore<0.16.0,>=0.15.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]) (0.15.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.16)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.1.16)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (1.24.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]) (8.2.2)\n",
      "Requirement already satisfied: uvicorn in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from sse-starlette<2.0.0,>=1.3.0->langserve[all]) (0.23.2)\n",
      "Requirement already satisfied: anyio in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from sse-starlette<2.0.0,>=1.3.0->langserve[all]) (3.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]) (1.8.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (0.9.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from httpcore<0.16.0,>=0.15.0->httpx>=0.23.0->langserve[all]) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from anyio->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (3.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.333->langserve[all]) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain>=0.0.333->langserve[all]) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]) (2.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from uvicorn->sse-starlette<2.0.0,>=1.3.0->langserve[all]) (8.0.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Serving with LangServe\n",
    "Now that we've built an application, we need to serve it. That's where LangServe comes in. \n",
    "LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to \n",
    "use LangChain, but in this guide we'll show how you can deploy your app with LangServe.\n",
    "\n",
    "While the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. \n",
    "We will be creating a Python file and then interacting with it from the command line.\n",
    "\n",
    "Install with:\n",
    "'''\n",
    "\n",
    "!pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d1736d4-e003-439b-94fd-bde855bfc446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nServer\\nTo create a server for our application we'll make a serve.py file. This will contain our logic for \\nserving our application. It consists of three things:\\n\\nThe definition of our chain that we just built above\\nOur FastAPI app\\nA definition of a route from which to serve the chain, which is done with langserve.add_routes\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Server\n",
    "To create a server for our application we'll make a serve.py file. This will contain our logic for \n",
    "serving our application. It consists of three things:\n",
    "\n",
    "The definition of our chain that we just built above\n",
    "Our FastAPI app\n",
    "A definition of a route from which to serve the chain, which is done with langserve.add_routes\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cfb395-d20c-4a14-8d8d-9e5191fc9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Playground\n",
    "Every LangServe service comes with a simple built-in UI for configuring and invoking the application \n",
    "with streaming output and visibility into intermediate steps. Head to http://localhost:8000/agent/playground/ \n",
    "to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should \n",
    "respond same as before.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e39fc03b-3f2f-4db2-a7a2-79f30a899d46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Doc' from 'typing_extensions' (/Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mClient\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mNow let's set up a client for programmatically interacting with our service. We can easily do this with \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mthe [langserve.RemoteRunnable](/docs/langserve#client). Using this, we can interact with the served chain \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mas if it were running client-side.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangserve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RemoteRunnable\n\u001b[1;32m     10\u001b[0m remote_chain \u001b[38;5;241m=\u001b[39m RemoteRunnable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8000/agent/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m remote_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow can langsmith help with testing?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langserve/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Main entrypoint into package.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis is the ONLY public interface into the package. All other modules are\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mto be considered private and subject to change without notice.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangserve\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m APIHandler\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangserve\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RemoteRunnable\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangserve\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CustomUserType\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langserve/api_handler.py:24\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isclass\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Any,\n\u001b[1;32m     10\u001b[0m     AsyncIterator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     Union,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPException\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsonable_encoder\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestValidationError\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/__init__.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.109.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m status \u001b[38;5;28;01mas\u001b[39;00m status\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI \u001b[38;5;28;01mas\u001b[39;00m FastAPI\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackground\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackgroundTasks \u001b[38;5;28;01mas\u001b[39;00m BackgroundTasks\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatastructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UploadFile \u001b[38;5;28;01mas\u001b[39;00m UploadFile\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/applications.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Any,\n\u001b[1;32m      4\u001b[0m     Awaitable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     Union,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m routing\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatastructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Default, DefaultPlaceholder\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception_handlers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     http_exception_handler,\n\u001b[1;32m     20\u001b[0m     request_validation_exception_handler,\n\u001b[1;32m     21\u001b[0m     websocket_request_validation_exception_handler,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/routing.py:22\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum, IntEnum\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     Any,\n\u001b[1;32m     10\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     Union,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m params\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     ModelField,\n\u001b[1;32m     25\u001b[0m     Undefined,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     lenient_issubclass,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatastructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Default, DefaultPlaceholder\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/params.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Sequence, Union\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Example\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfields\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FieldInfo\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotated, deprecated\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/openapi/models.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Iterable, List, Optional, Set, Type, Union\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     PYDANTIC_V2,\n\u001b[1;32m      6\u001b[0m     CoreSchema,\n\u001b[1;32m      7\u001b[0m     GetJsonSchemaHandler,\n\u001b[1;32m      8\u001b[0m     JsonSchemaValue,\n\u001b[1;32m      9\u001b[0m     _model_rebuild,\n\u001b[1;32m     10\u001b[0m     with_info_plain_validator_function,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyUrl, BaseModel, Field\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/_compat.py:20\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     Any,\n\u001b[1;32m      7\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     Union,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestErrorModel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncEx, ModelNameMap, UnionType\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, create_model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fastapi/exceptions.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPException \u001b[38;5;28;01mas\u001b[39;00m StarletteHTTPException\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstarlette\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebSocketException \u001b[38;5;28;01mas\u001b[39;00m StarletteWebSocketException\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotated, Doc  \u001b[38;5;66;03m# type: ignore [attr-defined]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHTTPException\u001b[39;00m(StarletteHTTPException):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    An HTTP exception you can raise in your own code to show errors to the client.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Doc' from 'typing_extensions' (/Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Client\n",
    "Now let's set up a client for programmatically interacting with our service. We can easily do this with \n",
    "the [langserve.RemoteRunnable](/docs/langserve#client). Using this, we can interact with the served chain \n",
    "as if it were running client-side.\n",
    "'''\n",
    "\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
    "remote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e553c-0281-45f7-924b-e5cb316e393d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
