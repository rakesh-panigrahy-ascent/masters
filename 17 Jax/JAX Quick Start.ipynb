{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03faa7c6-5ab8-4414-93a2-64282382374d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b623ff-8547-489b-b0c5-41d3756e74e0",
   "metadata": {},
   "source": [
    "### Multiplying Matrices\n",
    "We’ll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see Common Gotchas in JAX.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4949e4a8-48ae-464b-8f8a-2e7e8f5cdb7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n",
      "2024-02-24 23:15:05.552129: W pjrt_plugin/src/mps_client.cc:563] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0db6438-065c-41a6-a1e5-bfc2bc496c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.4 ms ± 18.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa52926-ae2e-4757-b08c-ec60155cce75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.3 ms ± 7.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d26cb6b-9978-4174-af25-ff993512395c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.5 ms ± 432 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c0824-ff6b-4719-8082-b836d497bfec",
   "metadata": {},
   "source": [
    "f you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU. See Is JAX faster than NumPy? for more comparison of performance characteristics of NumPy and JAX\n",
    "\n",
    "JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there are three main ones:\n",
    "\n",
    "jit(), for speeding up your code\n",
    "\n",
    "grad(), for taking derivatives\n",
    "\n",
    "vmap(), for automatic vectorization or batching.\n",
    "\n",
    "Let’s go over these, one-by-one. We’ll also end up composing these in interesting ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28cd17e-6923-42cd-9e79-84065f5df322",
   "metadata": {},
   "source": [
    "### Using jit() to speed up functions\n",
    "JAX runs transparently on the GPU or TPU (falling back to CPU if you don’t have one). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the @jit decorator to compile multiple operations together using XLA. Let’s try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4f9900-9bb8-4144-b217-3429140218e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.41 ms ± 87 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0897b4-773c-4461-98ab-8d12f8899d0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can speed it up with @jit, which will jit-compile the first time selu is called and will be cached thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feaddd4e-dae3-4b50-b03f-de7a97f8cfc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 µs ± 15.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcf06f-982c-4a9d-a6f1-8adc7f22ba70",
   "metadata": {},
   "source": [
    "### Taking derivatives with grad()\n",
    "In addition to evaluating numerical functions, we also want to transform them. One transformation is automatic differentiation. In JAX, just like in Autograd, you can compute gradients with the grad() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ebaff1a-16e3-48b6-8b24-78fa4bc9c8f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661194 0.10499358]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cfdcc-4da3-4a7c-aa0c-8dc67311ceb6",
   "metadata": {},
   "source": [
    "Let’s verify with finite differences that our result is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4efc626e-aa28-47d2-a800-1d7339e59b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2501011  0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x):\n",
    "  eps = 1e-3\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8ac6c-4cce-4a07-bdc6-dd94cff7e095",
   "metadata": {},
   "source": [
    "Taking derivatives is as easy as calling grad(). grad() and jit() compose and can be mixed arbitrarily. In the above example we jitted sum_logistic and then took its derivative. We can go further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deee940f-f65b-4a3a-b483-a4e2af89e8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.035325583\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3972f1-a386-4f81-8cba-16f597a90272",
   "metadata": {},
   "source": [
    "For more advanced autodiff, you can use jax.vjp() for reverse-mode vector-Jacobian products and jax.jvp() for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here’s one way to compose them to make a function that efficiently computes full Hessian matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e02badcd-cbe7-4148-9ff4-1a6c9268567e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69202cce-3f12-41fc-b708-be209dfa1f8d",
   "metadata": {},
   "source": [
    "### Auto-vectorization with vmap()\n",
    "JAX has one more transformation in its API that you might find useful: vmap(), the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a function’s primitive operations for better performance. When composed with jit(), it can be just as fast as adding the batch dimensions by hand.\n",
    "\n",
    "We’re going to work with a simple example, and promote matrix-vector products into matrix-matrix products using vmap(). Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "917a669f-2edb-4423-bf28-0c898b281db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "  return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889afa7b-1984-4f15-86cd-f2eaf7a7ae21",
   "metadata": {},
   "source": [
    "Given a function such as apply_matrix, we can loop over a batch dimension in Python, but usually the performance of doing so is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb3436a-6451-4912-8532-636b64b4f076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "3.64 ms ± 194 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fdc2ca-64ec-4fa1-85e6-4a5e3e21c141",
   "metadata": {},
   "source": [
    "We know how to batch this operation manually. In this case, jnp.dot handles extra batch dimensions transparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1dc24cf-7dec-4a2c-8b9d-d62bcbf57de6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "354 µs ± 37.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1e9f8-82f9-4151-9f7c-c072ad43918c",
   "metadata": {},
   "source": [
    "However, suppose we had a more complicated function without batching support. We can use vmap() to add batching support automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8623436-7e82-473e-aea7-c9bf9f8d05a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "The slowest run took 7.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "718 µs ± 752 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "  return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2507b86-0641-4989-851a-dd33c0bc2237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
