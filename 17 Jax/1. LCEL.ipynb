{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb855219-aa71-4151-a0d3-fb1ad5f30a27",
   "metadata": {},
   "source": [
    "## Langchain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb916d83-be09-4e21-bf98-242cf245fc96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s a short ice cream-related joke for you:\\n\\nWhy did the ice cream cone go to the party?\\n\\nBecause it was feeling a little \"crunchy\" and wanted to spread some \"smile\"!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "# model = ChatOpenAI(model=\"gpt-4\")\n",
    "model = Ollama(model=\"llama2\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f08905-1fdf-4b68-af03-cf25a886472a",
   "metadata": {},
   "source": [
    "Notice this line of this code, where we piece together then different components into a single chain using LCEL:\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "### 1. Prompt\n",
    "prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8224ca06-a25d-47fa-af0b-dca5bc1434cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b63b7dac-72a4-4e48-9e08-1aac13217529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='tell me a short joke about ice cream')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09c667a9-e1a3-40b6-bf6a-251c568701ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: tell me a short joke about ice cream'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cdeb34-f18d-4957-827b-afb558914376",
   "metadata": {},
   "source": [
    "### 2. Model\n",
    "The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2d4c915-e19e-4890-b32d-cafbec923677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s one:\\n\\nWhy did the ice cream cone go to the party?\\nBecause it was feeling \"sweet\"!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1284098-61d8-4c15-a331-5a97a851734e",
   "metadata": {},
   "source": [
    "### 3. Output parser\n",
    "And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c189b81e-2d02-4059-818d-253214e815aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s one:\\n\\nWhy did the ice cream cone go to the party?\\nBecause it was feeling \"sweet\"!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee65a7b-d894-4e0a-93d2-7b57373f1137",
   "metadata": {},
   "source": [
    "### Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de9aea15-3af1-4bf5-abc7-51ab07f9029d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSure, here\\'s one:\\n\\nWhy did the ice cream cone go to the party?\\n\\nBecause it was feeling \"frosty\" and wanted to cool off with some new friends!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\"topic\": \"ice cream\"}\n",
    "\n",
    "prompt.invoke(input)\n",
    "# > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n",
    "\n",
    "(prompt | model).invoke(input)\n",
    "# > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ceac71-3c19-44ac-9f86-60ca06dae3a2",
   "metadata": {},
   "source": [
    "### RAG Search Example\n",
    "\n",
    "For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6aca22d5-4ea4-4349-b6c0-b66696c1c0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (0.1.4)\n",
      "Requirement already satisfied: docarray in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (0.40.0)\n",
      "Requirement already satisfied: tiktoken in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (0.0.16)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (0.1.16)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from docarray) (3.9.12)\n",
      "Requirement already satisfied: rich>=13.1.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from docarray) (13.7.0)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from docarray) (2.31.0.20240125)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.0)\n",
      "\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/typing_extensions-4.7.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain docarray tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18fa4e03-9d8a-4ba1-ae38-1e5005d4cb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb4c27b9-2afe-4956-8375-edfbbd546471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OllamaEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4c94cf4-1747-42d1-a169-ad685ce193e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "394ffad2-8cbe-4771-8dcf-9c3e64b41d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = Ollama(model=\"llama2\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f03ecf3-92c5-4d34-9e8f-f568f9e28d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3cb2de3-e11c-4a21-95dd-544549b5898d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea196d8e-4388-4bca-9470-a43e1f8a4184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, I can infer that Harrison worked at Kensho. The sentence \"Harrison worked at Kensho\" appears in one of the documents, so it is likely that Harrison worked at this location.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957ec511-49a6-49be-a518-9f3e1f6ad5f4",
   "metadata": {},
   "source": [
    "To explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.\n",
    "\n",
    "As a preliminary step, we‚Äôve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "139fee60-fa01-444f-bf4d-6f0f08664f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='bears like to eat honey'),\n",
       " Document(page_content='harrison worked at kensho')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae1c02-265e-48d5-a38b-dee4a1d6b6bf",
   "metadata": {},
   "source": [
    "We then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user‚Äôs question:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dca7261b-bc0c-4a6c-9fdf-ed3f6126c4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42dedc47-d15f-4ce3-902e-2e37251ba482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To review, the complete chain is:\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "chain = setup_and_retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b52d5-7013-494d-932f-3bb65476fc77",
   "metadata": {},
   "source": [
    "With the flow being:\n",
    "\n",
    "* The first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user‚Äôs original question. To pass on the question, we use RunnablePassthrough to copy this entry.\n",
    "* Feed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue.\n",
    "* The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object.\n",
    "* Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4da87-0c68-4fa2-a10c-86613222b6a3",
   "metadata": {},
   "source": [
    "### Stream\n",
    "If we want to stream results instead, we‚Äôll need to change our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17f0aec3-9185-4683-b490-c81c2e577cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, I would answer \"No\" to the question \"Does Harrison like ice cream?\" because there is no information in the given documents to suggest that Harrison likes ice cream."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294281b-8d65-4fa7-b576-541d790bec32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batch\n",
    "If we want to run on a batch of inputs in parallel, we‚Äôll again need a new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d46e40be-538a-4e59-b6ad-3c96474464cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Based on the provided context, I must politely challenge the assumption in the question. The context does not provide any information about bears or ice cream, and therefore cannot be used to answer the question.\\n\\nThe first document mentions \"honey,\" but does not mention anything about ice cream. The second document mentions \"Harrison\" working at a place called \"Kensho,\" but does not provide any information about ice cream either.\\n\\nTherefore, I cannot provide an answer to the question based on the provided context.',\n",
       " ' Hmm, interesting combination of words there! Based solely on the context you provided, I would say that the answer to the question \"spaghetti\" is... yes! üçù',\n",
       " ' Based on the provided context, the answer to the question \"dumplings\" is not directly related to the information provided. However, we can make an educated guess based on the context.\\n\\nSince Harrison worked at Kensho and bears like to eat honey, it\\'s possible that Harrison might have had something to do with the production or distribution of honey. Therefore, it\\'s likely that dumplings are also related to honey in some way, perhaps as a food item that is made with honey or contains honey as an ingredient.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc59faa-4288-48d3-97b7-3c9d15bf7f72",
   "metadata": {},
   "source": [
    "### Async\n",
    "If we need an asynchronous version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58426470-5a73-46e2-a197-9b91575e8cae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object RunnableSequence.ainvoke at 0x290896c40>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f77cda-0c72-4853-9af4-5abd3866d6d9",
   "metadata": {},
   "source": [
    "### LLM instead of chat model\n",
    "If we want to use a completion endpoint instead of a chat endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9395437-eef7-43bd-960b-121b30ba6ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSure, here\\'s one:\\n\\nWhy did the ice cream cone go to the party?\\n\\nBecause it was feeling a little \"frozen\" and wanted to thaw out!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "model = Ollama(model=\"llama2\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "llm_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9094281-0ba6-49e0-b02c-00ce4689e7ff",
   "metadata": {},
   "source": [
    "### Different model provider\n",
    "If we want to use Anthropic instead of OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c8b4e55b-e6be-4246-be1a-eb600c324c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain_community.chat_models import ChatAnthropic\n",
    "\n",
    "# anthropic = ChatAnthropic(model=\"claude-2\")\n",
    "# anthropic_chain = (\n",
    "#     {\"topic\": RunnablePassthrough()} \n",
    "#     | prompt \n",
    "#     | anthropic\n",
    "#     | output_parser\n",
    "# )\n",
    "\n",
    "# anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac294cb4-b39d-470d-a46a-fe55ded19a56",
   "metadata": {},
   "source": [
    "### Runtime configurability\n",
    "If we wanted to make the choice of chat model or LLM configurable at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38c156cb-fb71-4b78-9ef7-5a34a66f549e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "\n",
    "configurable_model = model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"), \n",
    "    default_key=\"chat_openai\", \n",
    "    openai=model,\n",
    "    anthropic=llm_chain,\n",
    ")\n",
    "configurable_chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | configurable_model \n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db4fe6-7e3d-4c83-b142-c0ae57997b3c",
   "metadata": {},
   "source": [
    "### Logging\n",
    "If we want to log our intermediate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a6c13df-ff3b-4c17-b058-429d7c29b3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "# anthropic_chain.invoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578a14f-72b4-4cf5-a129-6a12c1cff995",
   "metadata": {},
   "source": [
    "### Fallbacks\n",
    "If we wanted to add fallback logic, in case one model API is down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e517d237-60e4-452a-a72a-1f7112e7741a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Ah, a clever question! Based on the provided context, I must respectfully decline to answer your query about ice cream. You see, bears are not known to have a particular fondness for ice cream. In fact, they tend to prefer more savory and sweet treats like honey. So, I'm afraid ice cream is not something bears would enjoy.\\n\\nNow, if you'll excuse me, I must return to my work at Kensho. Harrison, the esteemed colleague I mentioned earlier, is no doubt busy with his duties there.\",\n",
       " \" Human: Hmm, interesting combination! Based on the context provided, I would say that spaghetti is not a likely food choice for bears. Bears are more likely to enjoy foods that are high in protein and fat, such as honey. However, if bears were to eat spaghetti, it would probably be a messy and difficult process since they don't have opposable thumbs or the ability to use utensils like humans do. So while it's not the most likely food choice for bears, it's possible that they could figure out a way to enjoy it with some creativity and determination!\",\n",
       " ' Hmm, interesting! Based on the given context, I would say that the answer to the question \"dumplings\" is likely related to Harrison working at Kensho. Here\\'s why:\\n\\n* Document(page_content=\\'harrison worked at kensho\\') suggests that Harrison works at a place called Kensho.\\n* Document(page_content=\\'bears like to eat honey\\') doesn\\'t provide any direct connection to dumplings.\\n\\nGiven these two pieces of information, it seems more likely that Harrison\\'s work at Kensho is related to dumplings in some way. Perhaps he works there as a chef or food scientist specializing in dumpling production? Or maybe Kensho is a company that produces dumpling-related products? Without more information, it\\'s difficult to say for sure!']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallback_chain = chain.with_fallbacks([llm_chain])\n",
    "\n",
    "fallback_chain.invoke(\"ice cream\")\n",
    "# await fallback_chain.ainvoke(\"ice cream\")\n",
    "fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74188b93-dfba-4cc4-99e4-fa5a21cfca5d",
   "metadata": {},
   "source": [
    "### Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a4b32de-72b2-4acc-aa3f-3f44c0aa52bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.chat_models import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, ConfigurableField\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\"\n",
    "os.environ['LANGCHAIN_API_KEY']='ls__971f18068e7e4536a62f1811c3aeb059'\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "openai = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "anthropic = model\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([anthropic])\n",
    "    .configurable_alternatives(\n",
    "        ConfigurableField(id=\"model\"),\n",
    "        default_key=\"chat_openai\",\n",
    "        openai=openai,\n",
    "        anthropic=anthropic,\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6dad0-3a9c-4fa4-bf3f-75e3e36a7a3f",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59429286-f934-4b94-8390-2dfdc5f034e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf9a70-c2a6-402f-bb1f-833560e10f31",
   "metadata": {},
   "source": [
    "### Input Schema\n",
    "A description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4eefd237-e894-40df-a90d-f1ea8234a7b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input schema of the chain is the input schema of its first part, the prompt.\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9d6a765-ac1e-4589-b410-2a64acfd9bca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30070afd-2c02-458d-81bb-7f407ca2bdfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OllamaInput',\n",
       " 'anyOf': [{'type': 'string'},\n",
       "  {'$ref': '#/definitions/StringPromptValue'},\n",
       "  {'$ref': '#/definitions/ChatPromptValueConcrete'},\n",
       "  {'type': 'array',\n",
       "   'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "     {'$ref': '#/definitions/HumanMessage'},\n",
       "     {'$ref': '#/definitions/ChatMessage'},\n",
       "     {'$ref': '#/definitions/SystemMessage'},\n",
       "     {'$ref': '#/definitions/FunctionMessage'},\n",
       "     {'$ref': '#/definitions/ToolMessage'}]}}],\n",
       " 'definitions': {'StringPromptValue': {'title': 'StringPromptValue',\n",
       "   'description': 'String prompt value.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'StringPromptValue',\n",
       "     'enum': ['StringPromptValue'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'AIMessage': {'title': 'AIMessage',\n",
       "   'description': 'A Message from an AI.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ai',\n",
       "     'enum': ['ai'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'HumanMessage': {'title': 'HumanMessage',\n",
       "   'description': 'A Message from a human.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'human',\n",
       "     'enum': ['human'],\n",
       "     'type': 'string'},\n",
       "    'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},\n",
       "   'required': ['content']},\n",
       "  'ChatMessage': {'title': 'ChatMessage',\n",
       "   'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'chat',\n",
       "     'enum': ['chat'],\n",
       "     'type': 'string'},\n",
       "    'role': {'title': 'Role', 'type': 'string'}},\n",
       "   'required': ['content', 'role']},\n",
       "  'SystemMessage': {'title': 'SystemMessage',\n",
       "   'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'system',\n",
       "     'enum': ['system'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['content']},\n",
       "  'FunctionMessage': {'title': 'FunctionMessage',\n",
       "   'description': 'A Message for passing the result of executing a function back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'function',\n",
       "     'enum': ['function'],\n",
       "     'type': 'string'},\n",
       "    'name': {'title': 'Name', 'type': 'string'}},\n",
       "   'required': ['content', 'name']},\n",
       "  'ToolMessage': {'title': 'ToolMessage',\n",
       "   'description': 'A Message for passing the result of executing a tool back to a model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'content': {'title': 'Content',\n",
       "     'anyOf': [{'type': 'string'},\n",
       "      {'type': 'array',\n",
       "       'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]}}]},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'tool',\n",
       "     'enum': ['tool'],\n",
       "     'type': 'string'},\n",
       "    'tool_call_id': {'title': 'Tool Call Id', 'type': 'string'}},\n",
       "   'required': ['content', 'tool_call_id']},\n",
       "  'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',\n",
       "   'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',\n",
       "   'type': 'object',\n",
       "   'properties': {'messages': {'title': 'Messages',\n",
       "     'type': 'array',\n",
       "     'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},\n",
       "       {'$ref': '#/definitions/HumanMessage'},\n",
       "       {'$ref': '#/definitions/ChatMessage'},\n",
       "       {'$ref': '#/definitions/SystemMessage'},\n",
       "       {'$ref': '#/definitions/FunctionMessage'},\n",
       "       {'$ref': '#/definitions/ToolMessage'}]}},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'ChatPromptValueConcrete',\n",
       "     'enum': ['ChatPromptValueConcrete'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['messages']}}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d58a8-4e93-41e0-8221-691e2b7d6410",
   "metadata": {},
   "source": [
    "### Output Schema\n",
    "A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a62d1ee3-1cfd-427b-8eb0-8d1539681523",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'OllamaOutput', 'type': 'string'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage\n",
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8991b6-0e70-4a1f-b448-9b526058c43f",
   "metadata": {},
   "source": [
    "### Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b5506360-efb9-4dc0-8f7d-36adbb90718a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure, here's one:\n",
      "\n",
      "Why did the bear go to the restaurant?\n",
      "\n",
      "Because he wanted to have a \"bear\"-licious meal!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38336ba8-9df4-4e25-9af0-708a9a3cef77",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7af38918-6ba5-4046-90ea-88d64dd40bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, here's one:\\n\\nWhy did the bear go to the vending machine?\\n\\nBecause he wanted to get his paws on some snacks!\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b70a7-c5f3-4b19-b4d7-69c3e788372c",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6a3ce56-096f-4036-9e55-79bff56825a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nWhy did the bear go to the vet?\\n\\nBecause he was feeling a little ruff!',\n",
       " \"\\nSure, here's one:\\n\\nWhy did the cat join a band? Because he wanted to be the purr-cussionist!\"]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851db378-0fbc-4686-bd21-4c54c6d1d5d4",
   "metadata": {},
   "source": [
    "You can set the number of concurrent requests by using the max_concurrency parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d97d2b23-810e-4335-b8bc-a4ee7e28b19d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure, here\\'s one:\\n\\nWhy did the bear go to the party?\\n\\nBecause he wanted to have a \"bear\"-ly good time!',\n",
       " '\\nWhy did the cat join a band? Because he wanted to be the purr-cussionist!']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bac26-cb85-4249-b82d-4496e8e6b634",
   "metadata": {},
   "source": [
    "### Async Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23ddc064-a6bc-434d-9ca4-de04eab3ccb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure! Here's one:\n",
      "\n",
      "Why did the bear go to the restaurant?\n",
      "\n",
      "Because he wanted to have a paws-itive dining experience!"
     ]
    }
   ],
   "source": [
    "async for s in chain.astream({\"topic\": \"bears\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ed11d-60f0-4379-a978-b72a02368c04",
   "metadata": {},
   "source": [
    "### Async Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ded854b6-5fbf-480e-9672-d04ca54a9504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSure, here\\'s one:\\n\\nWhy did the bear go to the party?\\n\\nBecause he wanted to have a \"bear-ly\" good time!'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bfead7-1683-49c8-bb77-d325ed099f6b",
   "metadata": {},
   "source": [
    "### Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d84ba451-c2d0-4607-b052-c649099540f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Sure, here's one:\\n\\nWhy did the bear go to the restaurant?\\n\\nBecause he heard the service was grrr-eat!\"]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.abatch([{\"topic\": \"bears\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29a6ea-7816-42b0-bb38-593259d81cd7",
   "metadata": {},
   "source": [
    "* Use async throughout the code (including async tools etc)\n",
    "* Propagate callbacks if defining custom functions / runnables.\n",
    "* Whenever using runnables without LCEL, make sure to call .astream() on LLMs rather than .ainvoke to force the LLM to stream tokens.\n",
    "\n",
    "Let‚Äôs define a new chain to make it more interesting to show off the astream_events interface (and later the astream_log interface).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a3d013c2-3253-4757-881a-c6417d7b0e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OllamaEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model.with_config(run_name=\"my_llm\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0aaf20-71bc-4586-9115-a1a73418c128",
   "metadata": {},
   "source": [
    "Now let‚Äôs use astream_events to get events from the retriever and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82783534-101f-442b-aa88-3a8dc6b8bc64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakesh.panigrahy/anaconda3/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Retrieved the following documents:\n",
      "[Document(page_content='harrison worked at kensho')]\n"
     ]
    }
   ],
   "source": [
    "async for event in retrieval_chain.astream_events(\n",
    "    \"where did harrison work?\", version=\"v1\", include_names=[\"Docs\", \"my_llm\"]\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"|\")\n",
    "    elif kind in {\"on_chat_model_start\"}:\n",
    "        print()\n",
    "        print(\"Streaming LLM:\")\n",
    "    elif kind in {\"on_chat_model_end\"}:\n",
    "        print()\n",
    "        print(\"Done streaming LLM.\")\n",
    "    elif kind == \"on_retriever_end\":\n",
    "        print(\"--\")\n",
    "        print(\"Retrieved the following documents:\")\n",
    "        print(event[\"data\"][\"output\"][\"documents\"])\n",
    "    elif kind == \"on_tool_end\":\n",
    "        print(f\"Ended tool: {event['name']}\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01848e4-5377-4c69-aebb-aaf80c99b81a",
   "metadata": {},
   "source": [
    "### Async Stream Intermediate Steps\n",
    "All runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence.\n",
    "\n",
    "This is useful to show progress to the user, to use intermediate results, or to debug your chain.\n",
    "\n",
    "You can stream all steps (default) or include/exclude steps by name, tags or metadata.\n",
    "\n",
    "This method yields JSONPatch ops that when applied in the same order as received build up the RunState."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3019ba8b-8977-4d1c-a715-2cf883a1c3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TypedDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLogEntry\u001b[39;00m(TypedDict):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"ID of the sub-run.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TypedDict' is not defined"
     ]
    }
   ],
   "source": [
    "class LogEntry(TypedDict):\n",
    "    id: str\n",
    "    \"\"\"ID of the sub-run.\"\"\"\n",
    "    name: str\n",
    "    \"\"\"Name of the object being run.\"\"\"\n",
    "    type: str\n",
    "    \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"\n",
    "    tags: List[str]\n",
    "    \"\"\"List of tags for the run.\"\"\"\n",
    "    metadata: Dict[str, Any]\n",
    "    \"\"\"Key-value pairs of metadata for the run.\"\"\"\n",
    "    start_time: str\n",
    "    \"\"\"ISO-8601 timestamp of when the run started.\"\"\"\n",
    "\n",
    "    streamed_output_str: List[str]\n",
    "    \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of this run.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "    end_time: Optional[str]\n",
    "    \"\"\"ISO-8601 timestamp of when the run ended.\n",
    "    Only available after the run has finished.\"\"\"\n",
    "\n",
    "\n",
    "class RunState(TypedDict):\n",
    "    id: str\n",
    "    \"\"\"ID of the run.\"\"\"\n",
    "    streamed_output: List[Any]\n",
    "    \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"\n",
    "    final_output: Optional[Any]\n",
    "    \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.\n",
    "    Only available after the run has finished successfully.\"\"\"\n",
    "\n",
    "    logs: Dict[str, LogEntry]\n",
    "    \"\"\"Map of run names to sub-runs. If filters were supplied, this list will\n",
    "    contain only the runs that matched the filters.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee887de-fb46-41da-a3b2-0827fe78048e",
   "metadata": {},
   "source": [
    "### JSONPatch chunks\n",
    "This is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f5822ac-3ed6-4913-8ba3-18024e86e1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '2521e62e-a82c-4c2e-bf74-e3b3f209be22',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '5af0d40f-edbe-4713-98cd-6a0533beed42',\n",
      "            'metadata': {},\n",
      "            'name': 'Docs',\n",
      "            'start_time': '2024-02-28T16:57:59.966+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Docs/final_output',\n",
      "  'value': {'documents': [Document(page_content='harrison worked at kensho')]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Docs/end_time',\n",
      "  'value': '2024-02-28T16:58:00.702+00:00'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'B'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'B'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ased'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' on'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based on'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' the'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based on the'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' provided'},\n",
      " {'op': 'replace', 'path': '/final_output', 'value': 'Based on the provided'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' context'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ','},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context,'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' I'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' can'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' infer'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' that'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Harrison'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked '\n",
      "           'at'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' a'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' company'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a company'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' called'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a company called'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' K'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a company called K'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ens'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a company called Kens'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a company called Kensho'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'},\n",
      " {'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': 'Based on the provided context, I can infer that Harrison worked at '\n",
      "           'a company called Kensho.'})\n",
      "----------------------------------------\n",
      "RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"where did harrison work?\", include_names=[\"Docs\"]\n",
    "):\n",
    "    print(\"-\" * 40)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138528c-1cd6-457d-8204-dad36d73aba2",
   "metadata": {},
   "source": [
    "### Streaming the incremental RunState\n",
    "You can simply pass diff=False to get incremental values of RunState. You get more verbose output with more repetitive parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af6f3a00-6a1e-4e9a-8c23-70e3ee676762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': None,\n",
      "                   'final_output': None,\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': None,\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': [],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'B',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B', 'ased'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B', 'ased', ' on'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B', 'ased', ' on', ' the'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B', 'ased', ' on', ' the', ' provided'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B', 'ased', ' on', ' the', ' provided', ' context'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context,',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B', 'ased', ' on', ' the', ' provided', ' context', ','],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked at',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked',\n",
      "                     ' at'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked at K',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked',\n",
      "                     ' at',\n",
      "                     ' K'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked at Kens',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked',\n",
      "                     ' at',\n",
      "                     ' K',\n",
      "                     'ens'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked at Kensho',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked',\n",
      "                     ' at',\n",
      "                     ' K',\n",
      "                     'ens',\n",
      "                     'ho'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked at Kensho.',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked',\n",
      "                     ' at',\n",
      "                     ' K',\n",
      "                     'ens',\n",
      "                     'ho',\n",
      "                     '.'],\n",
      " 'type': 'chain'})\n",
      "----------------------------------------------------------------------\n",
      "RunLog({'final_output': 'Based on the provided context, Harrison worked at Kensho.',\n",
      " 'id': 'ab9ae304-95b1-4994-90ed-a905bc1c289a',\n",
      " 'logs': {'Docs': {'end_time': '2024-02-28T16:58:37.748+00:00',\n",
      "                   'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},\n",
      "                   'id': '96940499-40a4-4cc3-b522-da152132a122',\n",
      "                   'metadata': {},\n",
      "                   'name': 'Docs',\n",
      "                   'start_time': '2024-02-28T16:58:37.069+00:00',\n",
      "                   'streamed_output': [],\n",
      "                   'streamed_output_str': [],\n",
      "                   'tags': ['map:key:context', 'FAISS', 'OllamaEmbeddings'],\n",
      "                   'type': 'retriever'}},\n",
      " 'name': 'RunnableSequence',\n",
      " 'streamed_output': ['B',\n",
      "                     'ased',\n",
      "                     ' on',\n",
      "                     ' the',\n",
      "                     ' provided',\n",
      "                     ' context',\n",
      "                     ',',\n",
      "                     ' Harrison',\n",
      "                     ' worked',\n",
      "                     ' at',\n",
      "                     ' K',\n",
      "                     'ens',\n",
      "                     'ho',\n",
      "                     '.',\n",
      "                     ''],\n",
      " 'type': 'chain'})\n"
     ]
    }
   ],
   "source": [
    "async for chunk in retrieval_chain.astream_log(\n",
    "    \"where did harrison work?\", include_names=[\"Docs\"], diff=False\n",
    "):\n",
    "    print(\"-\" * 70)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d16c9c-6dc5-4b65-b8f7-9d9413b2d07e",
   "metadata": {},
   "source": [
    "### Parallelism\n",
    "Let‚Äôs take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ffa341f5-b4f9-4c8e-a5be-189385a749f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")\n",
    "    | model\n",
    ")\n",
    "combined = RunnableParallel(joke=chain1, poem=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09323844-a5f1-48b8-8adb-3874d97438cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.6 ms, sys: 7.3 ms, total: 47.9 ms\n",
      "Wall time: 4.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, here\\'s one:\\n\\nWhy did the bear go to the restaurant?\\n\\nBecause he wanted to have a \"paws-itive\" dining experience!'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "40a7265b-a591-407c-94bc-5941feba7278",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 ms, sys: 6.54 ms, total: 36.2 ms\n",
      "Wall time: 3.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Bears in the woods, so strong and free,\\nA symbol of power, wild and serene.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "27a1c835-881c-4612-956c-415b61a34303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 525 ms, sys: 20 ms, total: 545 ms\n",
      "Wall time: 7.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'joke': 'Sure, here\\'s one:\\n\\nWhy did the bear go to the restaurant?\\n\\nBecause he wanted to have a \"paws-itive\" dining experience!',\n",
       " 'poem': 'In forests deep, where twilight reigns,\\nBears roam, their power sustains.'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb74412-53b7-4239-8dab-e74778b1fc3b",
   "metadata": {},
   "source": [
    "### Parallelism on batches\n",
    "Parallelism can be combined with other runnables. Let‚Äôs try to use parallelism with batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38a9c424-bca4-4785-afa6-6de0a5a1b6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 67.5 ms, sys: 12.2 ms, total: 79.7 ms\n",
      "Wall time: 7.87 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sure, here\\'s one:\\n\\nWhy did the bear go to the barber?\\n\\nBecause he wanted to get a \"bear-able\" haircut!',\n",
       " '\\nWhy did the cat join a band? Because he wanted to be the purr-cussionist!']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9db4c4d-4fb4-4558-bb04-858bdc970b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.4 ms, sys: 12.9 ms, total: 81.3 ms\n",
      "Wall time: 10.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sure! Here is a short poem about bears in two lines:\\n\\nGrizzly giants roam the land,\\nTheir furry coats a sight to stand.',\n",
       " 'Certainly! Here is a short 2-line poem about cats:\\n\\nFurry balls of joy, with eyes so bright,\\nPurring their way through the night.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dad5a646-3cf5-4e23-b4b3-ad25f58f9b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 391 ms, sys: 33.2 ms, total: 424 ms\n",
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'joke': '\\nSure, here\\'s one:\\n\\nWhy did the bear go to the party?\\n\\nBecause he wanted to have a \"bear\"-ly good time!',\n",
       "  'poem': 'In the woods, a bear roams free,\\nA symbol of power and wild beauty.'},\n",
       " {'joke': \"\\nSure, here's one:\\n\\nWhy did the cat join a band?\\n\\nBecause he wanted to be the purr-cussionist!\",\n",
       "  'poem': 'Certainly! Here is a short two-line poem about cats:\\n\\nFurry balls of joy, with eyes so bright\\nPurring in the sun, on a warm and cozy night'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "combined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f376454-dcec-4505-bf96-47a5518162cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
